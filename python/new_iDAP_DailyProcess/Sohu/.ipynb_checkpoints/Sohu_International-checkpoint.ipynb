{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 網站名稱：搜狐\n",
    "# 網址： https://www.sohu.com/c/8/1461\n",
    "# 爬取類型： 國際\n",
    "# 爬取範圍： 今日、昨日\n",
    "#############################################\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pymysql\n",
    "import json\n",
    "\n",
    "\n",
    "def WebCrawling(days=3):\n",
    "    host = '10.55.23.101'\n",
    "    port = 33060\n",
    "    user = 'root'\n",
    "    passwd = \"1234\"\n",
    "    db = 'idap'\n",
    "    web = \"Sohu_International\" \n",
    "    tag = \"International\"\n",
    "\n",
    "    targetUrl = \"https://v2.sohu.com/public-api/feed?scene=CATEGORY&sceneId=1461&page={}&size=20\"\n",
    "    baseUrl = \"https://www.sohu.com/a/{}_{}\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    conn = pymysql.connect(host=host, port=port,\n",
    "                           user=user, passwd=passwd, db=db)\n",
    "    cur = conn.cursor()\n",
    "    page = 1\n",
    "\n",
    "    while(True):\n",
    "        res = requests.get(targetUrl.format(page), headers=headers)\n",
    "        res.encoding = 'utf-8'\n",
    "        if res.status_code == 403:\n",
    "            return\n",
    "        if res.status_code == 200:\n",
    "            news = json.loads(res.text)\n",
    "            for new in news:\n",
    "                url = baseUrl.format(new[\"id\"], new[\"authorId\"])\n",
    "                title = new[\"title\"]\n",
    "\n",
    "                creationdate = datetime.now()\n",
    "                content = ''\n",
    "\n",
    "                contentRes = requests.get(url, headers=headers)\n",
    "                contentRes.encoding = 'utf-8'\n",
    "                if contentRes.status_code == 200:\n",
    "                    contentSoup = BeautifulSoup(contentRes.text, 'lxml')\n",
    "                    publishdate = contentSoup.select('#news-time')[0].text[0:10].replace('-', '')\n",
    "                    if publishdate < (datetime.today() - timedelta(days=days)).strftime('%Y%m%d'):\n",
    "                        return\n",
    "\n",
    "                    contents = contentSoup.select('article.article > p:not(.ql-align-center):not([data-role=\"editor-name\"])') + contentSoup.select('div.hidden-content > p:not(.ql-align-center)')\n",
    "                    content = ' '.join([c.text.strip() for c in contents])\n",
    "                    content = content.replace(\"返回搜狐，查看更多\", \"\")         \n",
    "                    contentRes.close()\n",
    "\n",
    "                    cur.execute('select count(1) from news_daily where url=%s',(url))\n",
    "                    if cur.fetchone()[0] == 0 :\n",
    "                        cur.execute('insert ignore into news_daily(web, title, content, tag, publishdate, url, creationdate)values(%s, %s, %s, %s, %s, %s, %s)', (web, title, content, tag, publishdate, url, creationdate))\n",
    "                        cur.execute('commit')\n",
    "                        c=c+1\n",
    "\n",
    "        res.close()\n",
    "        page = page + 1\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    host = 'pc89600059495s'\n",
    "    port = 33060\n",
    "    user = 'root'\n",
    "    passwd = \"1234\"\n",
    "    db = 'idap'\n",
    "\n",
    "    web = \"搜狐\"\n",
    "\n",
    "    WebCrawling()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = '10.55.23.101'\n",
    "port = 33060\n",
    "user = 'root'\n",
    "passwd = \"1234\"\n",
    "db = 'idap'\n",
    "web = \"Sohu_International\" \n",
    "tag = \"International\"\n",
    "\n",
    "targetUrl = \"https://v2.sohu.com/public-api/feed?scene=CATEGORY&sceneId=1461&page={}&size=20\"\n",
    "baseUrl = \"https://www.sohu.com/a/{}_{}\"\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36'\n",
    "}\n",
    "\n",
    "conn = pymysql.connect(host=host, port=port,\n",
    "                       user=user, passwd=passwd, db=db)\n",
    "cur = conn.cursor()\n",
    "page = 1\n",
    "\n",
    "while(page<20):\n",
    "    res = requests.get(targetUrl.format(page), headers=headers)\n",
    "    res.encoding = 'utf-8'\n",
    "    if res.status_code == 403:\n",
    "        return\n",
    "    if res.status_code == 200:\n",
    "        news = json.loads(res.text)\n",
    "        for new in news:\n",
    "            url = baseUrl.format(new[\"id\"], new[\"authorId\"])\n",
    "            title = new[\"title\"]\n",
    "\n",
    "            creationdate = datetime.now()\n",
    "            content = ''\n",
    "\n",
    "            contentRes = requests.get(url, headers=headers)\n",
    "            contentRes.encoding = 'utf-8'\n",
    "            if contentRes.status_code == 200:\n",
    "                contentSoup = BeautifulSoup(contentRes.text, 'lxml')\n",
    "                publishdate = contentSoup.select('#news-time')[0].text[0:10].replace('-', '')\n",
    "                if publishdate < (datetime.today() - timedelta(days=days)).strftime('%Y%m%d'):\n",
    "                    return\n",
    "\n",
    "                contents = contentSoup.select('article.article > p:not(.ql-align-center):not([data-role=\"editor-name\"])') + contentSoup.select('div.hidden-content > p:not(.ql-align-center)')\n",
    "                content = ' '.join([c.text.strip() for c in contents])\n",
    "                content = content.replace(\"返回搜狐，查看更多\", \"\")         \n",
    "                contentRes.close()\n",
    "\n",
    "                cur.execute('select count(1) from news_daily where url=%s',(url))\n",
    "                if cur.fetchone()[0] == 0 :\n",
    "                    cur.execute('insert ignore into news_daily(web, title, content, tag, publishdate, url, creationdate)values(%s, %s, %s, %s, %s, %s, %s)', (web, title, content, tag, publishdate, url, creationdate))\n",
    "                    cur.execute('commit')\n",
    "                    c=c+1\n",
    "\n",
    "    res.close()\n",
    "    page = page + 1\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
