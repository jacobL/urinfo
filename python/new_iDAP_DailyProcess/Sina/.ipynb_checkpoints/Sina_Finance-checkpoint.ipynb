{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception Sina_Finance:execute() first\n",
      "Sina_Finance  2021-02-01 07:32:24.791872  total: 0\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# 網站名稱：新浪\n",
    "# 網址： https://news.sina.com.cn/china/\n",
    "# 爬取類型： 財經\n",
    "# 爬取範圍： 今日、昨日\n",
    "#############################################\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pymysql\n",
    "import json\n",
    "\n",
    "def WebCrawling(days = 3):\n",
    "    host = '10.55.23.101'\n",
    "    port = 33060 \n",
    "    user = 'root'\n",
    "    passwd = \"1234\"\n",
    "    db = 'idap'\n",
    "    web = \"Sina_Finance\" \n",
    "    tag = \"Finance\"\n",
    "    \n",
    "    targetUrl = \"https://feed.sina.com.cn/api/roll/get?pageid=121&lid=1356&num=20&versionNumber=1.2.4&page={}&encode=utf-8&callback=feedCardJsonpCallback&_=1608859920688\"\n",
    "    page = 1\n",
    "    \n",
    "    try:\n",
    "        conn = pymysql.connect(host=host, port=port,user=user, passwd=passwd, db=db)\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        c=0\n",
    "    \n",
    "        while(True):\n",
    "            res = requests.get(targetUrl.format(page))\n",
    "            res.encoding = 'utf-8'\n",
    "            if res.status_code == 403:\n",
    "                return\n",
    "            if res.status_code == 200:\n",
    "                res_content = res.text[26:-14]\n",
    "                result_json = json.loads(res_content)\n",
    "                news = result_json[\"result\"][\"data\"]\n",
    "\n",
    "                for new in news:\n",
    "                    publishdate = datetime.fromtimestamp(int(new['ctime'])).strftime('%Y%m%d')\n",
    "                    if publishdate < (datetime.today() - timedelta(days=days)).strftime('%Y%m%d'):\n",
    "                        return\n",
    "\n",
    "                    url = new[\"url\"]\n",
    "                    title = new[\"title\"]\n",
    "\n",
    "                    creationdate = datetime.now()\n",
    "                    content = ''\n",
    "\n",
    "                    contentRes = requests.get(url)\n",
    "                    contentRes.encoding = 'utf-8'\n",
    "                    if contentRes.status_code == 200:\n",
    "                        contentSoup = BeautifulSoup(contentRes.text, 'html.parser')\n",
    "                        contents = contentSoup.select('#article > p')\n",
    "                        content = ' '.join([c.text.strip() for c in contents])\n",
    "                        contentRes.close()\n",
    "\n",
    "                        if content.strip() == \"\":\n",
    "                            continue\n",
    "                        cur.execute('select count(1) from news_daily where url=%s',(url))\n",
    "                        if cur.fetchone()[0] == 0 :\n",
    "                            cur.execute('insert ignore into news_daily(web, title, content, tag, publishdate, url, creationdate)values(%s, %s, %s, %s, %s, %s, %s)', (web, title, content, tag, publishdate, url, creationdate))\n",
    "                            cur.execute('commit')\n",
    "                            c=c+1\n",
    "\n",
    "            res.close()\n",
    "            page = page + 1\n",
    "\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print('Exception Sina_Finance:'+str(e))\n",
    "    print('Sina_Finance ',creationdate,' total:',c)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    WebCrawling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = '10.55.23.101'\n",
    "port = 33060 \n",
    "user = 'root'\n",
    "passwd = \"1234\"\n",
    "db = 'idap'\n",
    "web = \"Sina_Finance\" \n",
    "tag = \"Finance\"\n",
    "\n",
    "targetUrl = \"https://feed.sina.com.cn/api/roll/get?pageid=121&lid=1356&num=20&versionNumber=1.2.4&page={}&encode=utf-8&callback=feedCardJsonpCallback&_=1608859920688\"\n",
    "page = 1\n",
    "\n",
    "days=3\n",
    "conn = pymysql.connect(host=host, port=port,user=user, passwd=passwd, db=db)\n",
    "cur = conn.cursor()\n",
    "\n",
    "c=0\n",
    "\n",
    "while(True):\n",
    "    res = requests.get(targetUrl.format(page))\n",
    "    res.encoding = 'utf-8'\n",
    "    if res.status_code == 403:\n",
    "        break\n",
    "    if res.status_code == 200:\n",
    "        res_content = res.text[26:-14]\n",
    "        result_json = json.loads(res_content)\n",
    "        news = result_json[\"result\"][\"data\"]\n",
    "\n",
    "        for new in news:\n",
    "            publishdate = datetime.fromtimestamp(int(new['ctime'])).strftime('%Y%m%d')\n",
    "            if publishdate < (datetime.today() - timedelta(days=days)).strftime('%Y%m%d'):\n",
    "                break\n",
    "\n",
    "            url = new[\"url\"]\n",
    "            title = new[\"title\"]\n",
    "\n",
    "            creationdate = datetime.now()\n",
    "            content = ''\n",
    "\n",
    "            contentRes = requests.get(url)\n",
    "            contentRes.encoding = 'utf-8'\n",
    "            if contentRes.status_code == 200:\n",
    "                contentSoup = BeautifulSoup(contentRes.text, 'html.parser')\n",
    "                contents = contentSoup.select('#article > p')\n",
    "                content = ' '.join([c.text.strip() for c in contents])\n",
    "                contentRes.close()\n",
    "\n",
    "                if content.strip() == \"\":\n",
    "                    continue\n",
    "                cur.execute('select count(1) from news_daily where url=%s',(url))\n",
    "                if cur.fetchone()[0] == 0 :\n",
    "                    cur.execute('insert ignore into news_daily(web, title, content, tag, publishdate, url, creationdate)values(%s, %s, %s, %s, %s, %s, %s)', (web, title, content, tag, publishdate, url, creationdate))\n",
    "                    cur.execute('commit')\n",
    "                    c=c+1\n",
    "\n",
    "    res.close()\n",
    "    page = page + 1\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
