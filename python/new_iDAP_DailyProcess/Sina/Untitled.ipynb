{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 網站名稱：新浪\n",
    "# 網址： https://news.sina.com.cn/world/\n",
    "# 爬取類型： 國際\n",
    "# 爬取範圍： 今日、昨日\n",
    "#############################################\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pymysql\n",
    "import json\n",
    "\n",
    "\n",
    "def WebCrawling():\n",
    "    targetUrl = \"https://interface.sina.cn/news/get_news_by_channel_new_v2018.d.html?cat_1=51923&show_num=27&level=1,2&page={}&callback=newsloadercallback&_=1608858408342\"\n",
    "\n",
    "    conn = pymysql.connect(host=host, port=port,\n",
    "                           user=user, passwd=passwd, db=db)\n",
    "    cur = conn.cursor()\n",
    "    page = 1\n",
    "\n",
    "    while(True):\n",
    "        res = requests.get(targetUrl.format(page))\n",
    "        res.encoding = 'utf-8'\n",
    "        if res.status_code == 403:\n",
    "            return\n",
    "        if res.status_code == 200:\n",
    "            res_content = res.text[19:-1]\n",
    "            result_json = json.loads(res_content)\n",
    "            news = result_json[\"result\"][\"data\"]\n",
    "\n",
    "            for new in news:\n",
    "                publishdate = datetime.fromtimestamp(int(new['createtime'])).strftime('%Y%m%d')\n",
    "                if publishdate < (datetime.today() - timedelta(days=1)).strftime('%Y%m%d'):\n",
    "                    return\n",
    "\n",
    "                url = new[\"url\"]\n",
    "                title = new[\"title\"]\n",
    "\n",
    "                creationdate = datetime.now()\n",
    "                content = ''\n",
    "\n",
    "                contentRes = requests.get(url)\n",
    "                contentRes.encoding = 'utf-8'\n",
    "                if contentRes.status_code == 200:\n",
    "                    contentSoup = BeautifulSoup(contentRes.text, 'lxml')\n",
    "                    contents = contentSoup.select('#article > p')\n",
    "                    content = ' '.join([c.text.strip() for c in contents])\n",
    "                contentRes.close()\n",
    "\n",
    "                if content.strip() == \"\":\n",
    "                    continue\n",
    "\n",
    "                # cur.execute('insert ignore into news(web, title, content, publishdate, url, creationdate)values(%s, %s, %s, %s, %s, %s)',\n",
    "                #             (web, title, content, publishdate, url, creationdate))\n",
    "                # cur.execute('commit')\n",
    "\n",
    "                print(\"============================================================\")\n",
    "                print(publishdate, title, url, content, creationdate)\n",
    "                print(\"============================================================\")\n",
    "\n",
    "        res.close()\n",
    "        page = page + 1\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    host = 'pc89600059495s'\n",
    "    port = 33060\n",
    "    user = 'root'\n",
    "    passwd = \"1234\"\n",
    "    db = 'idap'\n",
    "\n",
    "    web = \"新浪\"\n",
    "\n",
    "    WebCrawling()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
