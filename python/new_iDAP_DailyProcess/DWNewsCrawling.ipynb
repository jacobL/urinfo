{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pymysql\n",
    "from datetime import datetime, timedelta,timezone  \n",
    "import opencc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DW  2021/04/27 17:32:53  total: 56\n"
     ]
    }
   ],
   "source": [
    "def WebCrawling():\n",
    "    host = '10.55.23.101'\n",
    "    port = 33060\n",
    "    user = 'root'\n",
    "    passwd = \"1234\"\n",
    "    db = 'idap'\n",
    "\n",
    "    showPrintMSG = 1 # 0:不呈現，1:呈現 debug mode\n",
    "    archiveDate = 10\n",
    "    web = 'dw'\n",
    "    language = 'zh-cn'\n",
    "    sleep_sec = 2\n",
    "    lang_src=language\n",
    "    lang_tgt='zh-tw'\n",
    "    \n",
    "    baseUrl = \"https://www.dw.com/\"\n",
    "    targetUrl = \"https://www.dw.com/zh/在线报导/{}\"\n",
    "    targetArr = [\"s-9058\", \"非常德国/s-101347\", \"时政风云/s-1681\", \"评论分析/s-100993\", \"经济纵横/s-1682\", \"科技环境/s-1686\"]\n",
    "    header = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87 Safari/537.36',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    }\n",
    "\n",
    "    conn = pymysql.connect(host=host, port=port,user=user, passwd=passwd, db=db)\n",
    "    cur = conn.cursor()\n",
    "    cc = opencc.OpenCC('s2t')\n",
    "    deleteFromDate = datetime.strftime(datetime.now() - timedelta(archiveDate), '%Y%m%d')\n",
    "    tag = 'industry'\n",
    "    dt1 = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "    creationdate = dt1.astimezone(timezone(timedelta(hours=8))).strftime('%Y/%m/%d %H:%M:%S')\n",
    "    c = 0\n",
    "    for t in targetArr:\n",
    "        res = requests.get(targetUrl.format(t), headers=header)\n",
    "        res.encoding = 'utf-8'\n",
    "        if res.status_code == 200:\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "            news = soup.select('.news')\n",
    "            for new in news:\n",
    "\n",
    "                title = cc.convert(new.select('a > h2')[0].text)\n",
    "                url = baseUrl + new.select('a')[0].get('href').strip()\n",
    "                cur.execute('select count(1) from news_daily where url=%s',(url))\n",
    "                if cur.fetchone()[0] == 0 :\n",
    "                    \n",
    "\n",
    "                    try:\n",
    "                        contentRes = requests.get(url, timeout=5)\n",
    "                        contentRes.encoding = 'utf-8'\n",
    "                        if contentRes.status_code == 200:\n",
    "                            contentSoup = BeautifulSoup(contentRes.text, 'html.parser')\n",
    "\n",
    "                            publishdate = contentSoup.select('.smallList > li')[0].text.replace(\"日期\", \"\").strip()\n",
    "                            publishdate = datetime.strptime(publishdate, '%d.%m.%Y').strftime('%Y%m%d')\n",
    "                            #if publishdate < (datetime.today() - timedelta(days=1)).strftime('%Y%m%d'):\n",
    "                            #    continue\n",
    "                            if int(publishdate) > int(deleteFromDate) :\n",
    "                                \n",
    "                                bodyContent = contentSoup.select('#bodyContent')[0]\n",
    "                                contents = bodyContent.select('p')\n",
    "                                content = cc.convert(''.join([c.text.strip() for c in contents]))\n",
    "                                cur.execute('insert ignore into news_daily(web, title, content, publishdate, url, creationdate)values(%s,%s,%s,%s,%s,%s)',(web, title, content, publishdate, url, creationdate))\n",
    "                                cur.execute('commit')\n",
    "                                c = c + 1\n",
    "\n",
    "                    except requests.exceptions.RequestException as e:\n",
    "                        print('Exception DWNewsCrawling:'+str(e)) \n",
    "    print('DW ',creationdate,' total:',c)\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    WebCrawling()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
