{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pymysql\n",
    "from datetime import datetime, timedelta\n",
    "from google_trans_new import google_translator\n",
    "import time\n",
    "import random\n",
    "\n",
    "def daterange(date1, date2):\n",
    "    for n in range(int((date2 - date1).days)+1):\n",
    "        yield date1 + timedelta(n)\n",
    "        \n",
    "def gl_translator(text, lang_src, lang_tgt) :\n",
    "    url_suffix_list = ['ac','ad','ae','al','am','as','at','az','ba','be','bf','bg','bi','bj','bs','bt','by','ca','cat','cc','cd','cf','cg','ch','ci','cl','cm','cn','co.ao','co.bw','co.ck','co.cr','co.id','co.il','co.in','co.jp','co.ke','co.kr','co.ls','co.ma','co.mz','co.nz','co.th','co.tz','co.ug','co.uk','co.uz','co.ve','co.vi','co.za','co.zm','co.zw','co','com.af','com.ag','com.ai','com.ar','com.au','com.bd','com.bh','com.bn','com.bo','com.br','com.bz','com.co','com.cu','com.cy','com.do','com.ec','com.eg','com.et','com.fj','com.gh','com.gi','com.gt','com.hk','com.jm','com.kh','com.kw','com.lb','com.lc','com.ly','com.mm','com.mt','com.mx','com.my','com.na','com.ng','com.ni','com.np','com.om','com.pa','com.pe','com.pg','com.ph','com.pk','com.pr','com.py','com.qa','com.sa','com.sb','com.sg','com.sl','com.sv','com.tj','com.tr','com.tw','com.ua','com.uy','com.vc','com.vn','com','cv','cx','cz','de','dj','dk','dm','dz','ee','es','eu','fi','fm','fr','ga','ge','gf','gg','gl','gm','gp','gr','gy','hn','hr','ht','hu','ie','im','io','iq','is','it','je','jo','kg','ki','kz','la','li','lk','lt','lu','lv','md','me','mg','mk','ml','mn','ms','mu','mv','mw','ne','nf','nl','no','nr','nu','pl','pn','ps','pt','ro','rs','ru','rw','sc','se','sh','si','sk','sm','sn','so','sr','st','td','tg','tk','tl','tm','tn','to','tt','us','vg','vu','ws']\n",
    "    isSuccess = False\n",
    "    url_suffix_inx = random.randint(0, len(url_suffix_list)-1)\n",
    "    content_tw = ''\n",
    "    while isSuccess == False :\n",
    "        try :\n",
    "            translator = google_translator(url_suffix=url_suffix_list[url_suffix_inx])        \n",
    "            content_tw = translator.translate(text, lang_src=lang_src, lang_tgt=lang_tgt)\n",
    "            isSuccess = True\n",
    "        except Exception as e:\n",
    "            url_suffix_inx = random.randint(0, len(url_suffix_list)-1)\n",
    "    return content_tw\n",
    "\n",
    "def WebCrawling(days=5):\n",
    "    host = '10.55.23.101'\n",
    "    port = 33060\n",
    "    user = 'root'\n",
    "    passwd = \"1234\"\n",
    "    db = 'idap'\n",
    "    \n",
    "    showPrintMSG = 1 # 0:不呈現，1:呈現 debug mode\n",
    "    archiveDate = 10\n",
    "    web = 'nhk'\n",
    "    language = 'jp'\n",
    "    sleep_sec = 2\n",
    "    lang_src=language\n",
    "    lang_tgt='zh-tw'\n",
    "    tag = ''\n",
    "\n",
    "    targetUrl = \"https://www3.nhk.or.jp/news/json16/new_{}.json\"\n",
    "    baseUrl = \"https://www3.nhk.or.jp/news/\"\n",
    "    \n",
    "    \n",
    "    conn = pymysql.connect(host=host, port=port,user=user, passwd=passwd, db=db)\n",
    "    cur = conn.cursor()\n",
    "    c=0\n",
    "    for page in range(1, 11): # 超過10就404\n",
    "        try:\n",
    "            res = requests.get(targetUrl.format(str(page).zfill(3)))\n",
    "            #print(targetUrl.format(str(page).zfill(3)))\n",
    "            res.encoding = 'utf-8'\n",
    "            creationdate = datetime.now()\n",
    "            if res.status_code == 200:\n",
    "                result_json = res.json()\n",
    "\n",
    "                for item in result_json['channel']['item']:\n",
    "                    title = item['title']\n",
    "                    originUrl = item['link']\n",
    "                    url = baseUrl + originUrl\n",
    "                    cur.execute('select count(1) from news_daily_source where url=%s',(url))\n",
    "                    if cur.fetchone()[0] == 0 :\n",
    "                        creationdate = datetime.now()\n",
    "\n",
    "                        contentRes = requests.get(url)\n",
    "                        contentRes.encoding = 'utf-8'\n",
    "                        if contentRes.status_code == 200: \n",
    "                            soup = BeautifulSoup(contentRes.text, 'html.parser')\n",
    "                            publishdate = (soup.select('time')[0].get('datetime'))[0:10].replace('-', '')\n",
    "                            if publishdate < (datetime.today() - timedelta(days=days)).strftime('%Y%m%d'):\n",
    "                                return\n",
    "                            c=c+1\n",
    "                            content = soup.select('div.content--detail-body')[0].text \n",
    "                            title_tw = gl_translator(title, lang_src, lang_tgt)\n",
    "                            content_tw = gl_translator(content, lang_src, lang_tgt)\n",
    "                            #print(publishdate)\n",
    "                            #print(page,url)\n",
    "                            #print(title_tw,' ',title)\n",
    "                            #print(content)\n",
    "                            #print(content_tw+'\\n')\n",
    "\n",
    "                            cur.execute('insert into news_daily_source(web,title,title_tw,content,content_tw,publishdate,url,creationdate,language)values(%s,%s,%s,%s,%s,%s,%s,%s,%s)',(web,title,title_tw,content,content_tw, publishdate,url,creationdate,language))\n",
    "                            cur.execute('commit')\n",
    "            else :\n",
    "                print('res.status_code:',res.status_code)\n",
    "        except Exception as e:\n",
    "            print('Exception NHKNewsCrawling:'+str(e)) \n",
    "    print('NHKNews ',creationdate,' total:',c)\n",
    "    cur.close()\n",
    "    conn.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    WebCrawling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jacob.liang\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\site-packages\\\\pymysql\\\\__init__.py'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path = os.path.abspath(pymysql.__file__)\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
